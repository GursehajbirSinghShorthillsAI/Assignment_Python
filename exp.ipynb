{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Sample_file/sample.pdf successfully loaded.\n",
      "Text extracted and saved at: /home/shtlp_0126/Desktop/Assignment_Python/Output_pdf/text/extracted_text.txt\n",
      "Images extracted and saved at: /home/shtlp_0126/Desktop/Assignment_Python/Output_pdf/images\n",
      "Links extracted and saved at: /home/shtlp_0126/Desktop/Assignment_Python/Output_pdf/links/extracted_links.csv\n",
      "Table from page 1 saved at: /home/shtlp_0126/Desktop/Assignment_Python/Output_pdf/tables/table_page_1.csv\n",
      "Table from page 7 saved at: /home/shtlp_0126/Desktop/Assignment_Python/Output_pdf/tables/table_page_7.csv\n",
      "Table from page 8 saved at: /home/shtlp_0126/Desktop/Assignment_Python/Output_pdf/tables/table_page_8.csv\n",
      "Table from page 9 saved at: /home/shtlp_0126/Desktop/Assignment_Python/Output_pdf/tables/table_page_9.csv\n",
      "Table from page 14 saved at: /home/shtlp_0126/Desktop/Assignment_Python/Output_pdf/tables/table_page_14.csv\n",
      "Processing complete! Check the 'output' folder for results.\n",
      "File processing complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "import csv\n",
    "from PIL import Image\n",
    "from docx import Document  # For DOCX files\n",
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "import pptx\n",
    "from abc import ABC, abstractmethod\n",
    "import csv\n",
    "from PyPDF2 import PdfReader\n",
    "import json\n",
    "\n",
    "# # Output directory path\n",
    "# output_dir = os.path.join(os.getcwd(), 'output')\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# Path to the sample file\n",
    "file_path = 'Sample_file/sample.pdf'  # Update this for docx or pdf\n",
    "# Function to create segregated output directories\n",
    "def create_output_dirs(output_base_dir, file_type):\n",
    "    output_dir = os.path.join(output_base_dir, f\"Output_{file_type}\")\n",
    "    subfolders = ['text', 'images', 'tables', 'links', 'metadata']\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create subdirectories for text, images, tables, and links\n",
    "    for folder in subfolders:\n",
    "        os.makedirs(os.path.join(output_dir, folder), exist_ok=True)\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "class PDFLoader:\n",
    "    def __init__(self, file_path, output_dir):\n",
    "        self.file_path = file_path\n",
    "        self.output_dir = output_dir\n",
    "        self.doc = None\n",
    "\n",
    "    def load_file(self):\n",
    "        # Open the PDF file using PyMuPDF\n",
    "        self.doc = fitz.open(self.file_path)\n",
    "        print(f\"PDF {self.file_path} successfully loaded.\")\n",
    "        return self.doc\n",
    "\n",
    "    def extract_text(self):\n",
    "        # Extract text from each page\n",
    "        text = \"\"\n",
    "        for page_num in range(len(self.doc)):\n",
    "            page = self.doc.load_page(page_num)\n",
    "            text += page.get_text(\"text\")\n",
    "        \n",
    "        # Save text in the \"text\" subfolder\n",
    "        text_path = os.path.join(self.output_dir, 'text', 'extracted_text.txt')\n",
    "        with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Text extracted and saved at: {text_path}\")\n",
    "\n",
    "    def extract_links(self):\n",
    "        links = []\n",
    "        link_folder = os.path.join(self.output_dir, 'links')\n",
    "        link_csv_path = os.path.join(link_folder, \"extracted_links.csv\")\n",
    "        for page_num in range(len(self.doc)):\n",
    "            page = self.doc.load_page(page_num)\n",
    "            links_on_page = page.get_links()\n",
    "            for link in links_on_page:\n",
    "                if link.get(\"uri\"):\n",
    "                    links.append({\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"uri\": link.get(\"uri\")\n",
    "                    })\n",
    "\n",
    "        # Save links in the \"links\" subfolder\n",
    "        with open(link_csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Page\", \"URL\"])\n",
    "            for link in links:\n",
    "                writer.writerow([link[\"page\"], link[\"uri\"]])\n",
    "        print(f\"Links extracted and saved at: {link_csv_path}\")\n",
    "        return links\n",
    "    \n",
    "    def extract_images(self):\n",
    "        images = []\n",
    "        image_folder = os.path.join(self.output_dir, 'images')\n",
    "        for page_num in range(len(self.doc)):\n",
    "            page = self.doc.load_page(page_num)\n",
    "            image_list = page.get_images(full=True)\n",
    "            for image_index, img in enumerate(image_list):\n",
    "                xref = img[0]\n",
    "                base_image = self.doc.extract_image(xref)\n",
    "                img_bytes = base_image[\"image\"]\n",
    "                img_ext = base_image[\"ext\"]\n",
    "                img_path = os.path.join(image_folder, f\"image_page_{page_num + 1}_{image_index}.{img_ext}\")\n",
    "                with open(img_path, \"wb\") as img_file:\n",
    "                    img_file.write(img_bytes)\n",
    "                images.append(img_path)\n",
    "        print(f\"Images extracted and saved at: {image_folder}\")\n",
    "        return images\n",
    "    \n",
    "\n",
    "    def extract_tables(self):\n",
    "        # Extract tables (simplified, as PDFs aren't straightforward for table extraction)\n",
    "        tables = []\n",
    "        table_folder = os.path.join(self.output_dir, 'tables')\n",
    "        for page_num in range(len(self.doc)):\n",
    "            page = self.doc.load_page(page_num)\n",
    "            text = page.get_text(\"text\")\n",
    "            if \"table\" in text.lower():\n",
    "                table_data = {\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"content\": text\n",
    "                }\n",
    "                tables.append(table_data)\n",
    "                # Save each table in a CSV file\n",
    "                table_csv_path = os.path.join(table_folder, f\"table_page_{page_num + 1}.csv\")\n",
    "                with open(table_csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([text])\n",
    "                print(f\"Table from page {page_num + 1} saved at: {table_csv_path}\")\n",
    "        return tables\n",
    "\n",
    "    \n",
    "    def extract_detailed_metadata(self):\n",
    "        # Extract fonts, sizes, and other page-specific information\n",
    "        metadata = []\n",
    "        for page_num in range(len(self.doc)):\n",
    "            page = self.doc.load_page(page_num)\n",
    "            text_instances = page.get_text(\"dict\")[\"blocks\"]  # Get text blocks\n",
    "            page_metadata = []\n",
    "            for block in text_instances:\n",
    "                if block['type'] == 0:  # Type 0 means text block\n",
    "                    for line in block['lines']:\n",
    "                        for span in line['spans']:\n",
    "                            page_metadata.append({\n",
    "                                \"font\": span['font'],\n",
    "                                \"size\": span['size'],\n",
    "                                \"color\": span['color'],\n",
    "                                \"text\": span['text']\n",
    "                            })\n",
    "            metadata.append({\n",
    "                \"page\": page_num + 1,\n",
    "                \"fonts\": page_metadata\n",
    "            })\n",
    "        return metadata\n",
    "\n",
    "\n",
    "class DOCXLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.doc = Document(file_path)\n",
    "\n",
    "    def extract_detailed_metadata(self):\n",
    "        metadata = []\n",
    "        for para in self.doc.paragraphs:\n",
    "            para_metadata = []\n",
    "            for run in para.runs:\n",
    "                para_metadata.append({\n",
    "                    \"text\": run.text,\n",
    "                    \"font\": run.font.name,\n",
    "                    \"size\": run.font.size.pt if run.font.size else None,  # Get font size in points\n",
    "                    \"bold\": run.bold,\n",
    "                    \"italic\": run.italic,\n",
    "                    \"underline\": run.underline\n",
    "                })\n",
    "            metadata.append({\n",
    "                \"paragraph_text\": para.text,\n",
    "                \"run_metadata\": para_metadata\n",
    "            })\n",
    "        return metadata\n",
    "\n",
    "    def extract_text(self):\n",
    "        # Extract all the text from the DOCX document\n",
    "        text = \"\\n\".join([para.text for para in self.doc.paragraphs])\n",
    "        return text\n",
    "\n",
    "    def extract_links(self):\n",
    "        # Extract links from the DOCX document\n",
    "        links = []\n",
    "        for rel in self.doc.part.rels.values():\n",
    "            if \"hyperlink\" in rel.target_ref:\n",
    "                links.append(rel.target_ref)\n",
    "        return links\n",
    "\n",
    "    def extract_images(self):\n",
    "        # Extract images from the DOCX document\n",
    "        images = []\n",
    "        for rel in self.doc.part.rels.values():\n",
    "            if \"image\" in rel.target_ref:\n",
    "                img_path = os.path.join(output_dir, os.path.basename(rel.target_ref))\n",
    "                images.append(img_path)\n",
    "        return images\n",
    "\n",
    "    def extract_tables(self):\n",
    "        # Extract tables from DOCX document\n",
    "        tables = []\n",
    "        for table in self.doc.tables:\n",
    "            table_data = []\n",
    "            for row in table.rows:\n",
    "                row_data = [cell.text for cell in row.cells]\n",
    "                table_data.append(row_data)\n",
    "            tables.append(table_data)\n",
    "        return tables\n",
    "    \n",
    "\n",
    "output_base_dir = os.getcwd()  # Base directory for output\n",
    "\n",
    "\n",
    "def save_text_to_file(text, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_links_to_file(links, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"URL\"])\n",
    "        for link in links:\n",
    "            writer.writerow([link])\n",
    "\n",
    "def save_tables_to_csv(tables):\n",
    "    for index, table in enumerate(tables):\n",
    "        csv_path = os.path.join(output_dir, f\"table_{index + 1}.csv\")\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(table)\n",
    "\n",
    "def save_metadata_to_json(metadata, file_name):\n",
    "    json_path = os.path.join(output_dir, file_name)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(metadata, json_file, indent=4)\n",
    "\n",
    "# Determine if file is PDF or DOCX and process accordingly\n",
    "# if file_path.endswith('.pdf'):\n",
    "#     # Processing the PDF\n",
    "#     pdf_loader = PDFLoader(file_path)\n",
    "#     pdf_loader.load_file()\n",
    "\n",
    "#     # Extract text and save to a text file\n",
    "#     text = pdf_loader.extract_text()\n",
    "#     save_text_to_file(text, os.path.join(output_dir, \"extracted_text.txt\"))\n",
    "\n",
    "#     # Extract links and save to a CSV file\n",
    "#     links = pdf_loader.extract_links()\n",
    "#     save_links_to_file(links, os.path.join(output_dir, \"extracted_links.csv\"))\n",
    "\n",
    "#     # Extract images and save to the output directory\n",
    "#     images = pdf_loader.extract_images()\n",
    "#     print(f\"Images extracted and saved at: {images}\")\n",
    "\n",
    "#     # Extract tables and save to CSV files\n",
    "#     tables = pdf_loader.extract_tables()\n",
    "#     save_tables_to_csv(tables)\n",
    "\n",
    "#     detailed_metadata = pdf_loader.extract_detailed_metadata()\n",
    "#     save_metadata_to_json(detailed_metadata, \"pdf_detailed_metadata.json\")\n",
    "#     print(f\"Detailed PDF metadata saved to pdf_detailed_metadata.json\")\n",
    "\n",
    "# elif file_path.endswith('.docx'):\n",
    "#     # Processing the DOCX\n",
    "#     docx_loader = DOCXLoader(file_path)\n",
    "\n",
    "#     # Extract text and save to a text file\n",
    "#     text = docx_loader.extract_text()\n",
    "#     save_text_to_file(text, os.path.join(output_dir, \"extracted_text.txt\"))\n",
    "\n",
    "#     # Extract links and save to a CSV file\n",
    "#     links = docx_loader.extract_links()\n",
    "#     save_links_to_file(links, os.path.join(output_dir, \"extracted_links.csv\"))\n",
    "\n",
    "#     # Extract images and save to the output directory\n",
    "#     images = docx_loader.extract_images()\n",
    "#     print(f\"Images extracted and saved at: {images}\")\n",
    "\n",
    "#     # Extract tables and save to CSV files\n",
    "#     tables = docx_loader.extract_tables()\n",
    "#     save_tables_to_csv(tables)\n",
    "\n",
    "#     detailed_metadata = docx_loader.extract_detailed_metadata()\n",
    "#     save_metadata_to_json(detailed_metadata, \"docx_detailed_metadata.json\")\n",
    "#     print(f\"Detailed DOCX metadata saved to docx_detailed_metadata.json\")\n",
    "\n",
    "# Determine whether the file is PDF or DOCX and create output directories accordingly\n",
    "if file_path.endswith('.pdf'):\n",
    "    output_dir = create_output_dirs(output_base_dir, 'pdf')\n",
    "    pdf_loader = PDFLoader(file_path, output_dir)\n",
    "    pdf_loader.load_file()\n",
    "\n",
    "    # Perform extraction operations\n",
    "    pdf_loader.extract_text()\n",
    "    pdf_loader.extract_images()\n",
    "    pdf_loader.extract_links()\n",
    "    pdf_loader.extract_tables()\n",
    "\n",
    "elif file_path.endswith('.docx'):\n",
    "      output_dir = create_output_dirs(output_base_dir, 'docx')\n",
    "      docx_loader = DOCXLoader(file_path, output_dir)\n",
    "    \n",
    "      #Perform extraction operations for DOCX...\n",
    "      docx_loader.extract_text()\n",
    "      docx_loader.extract_images()\n",
    "      docx_loader.extract_links()\n",
    "      docx_loader.extract_tables()\n",
    "\n",
    "print(\"Processing complete! Check the 'output' folder for results.\")\n",
    "\n",
    "print(\"File processing complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
