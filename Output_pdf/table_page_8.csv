"Table 1: Performance comparison of different models under various noise ratios in English and
Chinese in RGB.
Chinese
Model
Noise Ratio
0.2
0.4
0.6
0.8
Avg.
gpt-4o-2024-08-06
98.3
98.0
96.6
87.7
95.2
Na¨ıve RAG
99.0
98.0
96.7
87.3
95.3
TurboRAG-composite w/o fine-tuning
98.3
96.3
93.7
79.0
91.8
TurboRAG-reordered w/o fine-tuning
98.0
96.7
93.3
81.3
92.3
TurboRAG-composite
99.0
97.3
96.0
86.7
94.8
TurboRAG-reordered
98.7
97.3
96.0
90.7
95.7
English
Model
Noise Ratio
0.2
0.4
0.6
0.8
Avg.
gpt-4o-2024-08-06
99.0
99.3
98.3
96.3
98.2
Na¨ıve RAG
99.7
99.3
99.3
94.3
98.2
TurboRAG-composite w/o fine-tuning
98.0
96.3
91.3
75.0
90.2
TurboRAG-reordered w/o fine-tuning
98.0
97.3
90.7
85.7
92.9
TurboRAG-composite
99.3
98.0
96.7
92.7
96.7
TurboRAG-reordered
99.0
98.3
96.0
93.7
96.8
Table 2: Performance of Naive RAG and TurboRAG on LongBench multi-document QA (subcate-
gories).
Subcategory Context
Token
Query
Token
Score
TTFT (ms)
Na¨ıve
Turbo
composite
Turbo
reordered Na¨ıve
Turbo
reordered Speedup
musique
16349
18.8
22.12
23.64
27.37
1610
171
9.4x
2wikimqa
7553
17.0
35.02
34.28
39.51
709
101
7.0x
dureader(zh)
10642
6.0
34.57
33.37
33.03
1007
116
8.7x
hotpotqa
13453
20.1
40.21
35.78
45.28
1333
147
9.1x
Avg.
11999
15.5
32.99
31.76
36.29
1165
134
8.6x
4.3
GENERAL CAPABILITY REGRESSION
To ensure that the non-standard attention masks and position IDs usded in fine-tuning does not
negatively affect the models’ general capabilities, we accomplished regression tests using the Open-
Compass1 benchmark on various mainstream tasks. As summarized in Table 3, the modifications
had minimal impact on the base capabilities of the models. TurboRAG-reordered showed strong
generalization across tasks, with no significant performance degradation compared to Na¨ıve RAG.
Table 3: Regression experiments of Na¨ıve RAG and TurboRAG. Evaluated by OpenCompass.
Model
MMLU
TriviaQA
GSM-8K
MATH
Na¨ıve RAG
69.57
56.90
79.12
39.54
TurboRAG-reordered
70.73
56.47
79.45
40.58
sub
+1.16
-0.43
+0.33
+1.04
1https://github.com/open-compass/opencompass
"
