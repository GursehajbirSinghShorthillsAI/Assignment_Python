"TURBORAG:
ACCELERATING
RETRIEVAL-
AUGMENTED GENERATION WITH PRECOMPUTED KV
CACHES FOR CHUNKED TEXT
Songshuo Lu
Hua Wang
Yutian Rong
Zhi Chen∗
Yaohua Tang†
Moore Threads AI
ABSTRACT
Current Retrieval-Augmented Generation (RAG) systems concatenate and process
numerous retrieved document chunks for prefill which requires a large volume of
computation, therefore leading to significant latency in time-to-first-token (TTFT).
To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a
novel RAG system that redesigns the inference paradigm of the current RAG sys-
tem by first pre-computing and storing the key-value (KV) caches of documents
offline, and then directly retrieving the saved KV cache for prefill. Hence, online
computation of KV caches is eliminated during inference. In addition, we pro-
vide a number of insights into the mask matrix and positional embedding mech-
anisms, plus fine-tune a pretrained language model to maintain model accuracy
of TurboRAG. Our approach is applicable to most existing large language models
and their applications without any requirement in modification of models and in-
ference systems. Experimental results across a suite of RAG benchmarks demon-
strate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional
RAG systems (on an average of 8.6x), but reserving comparable performance to
the standard RAG systems.
1
INTRODUCTION
Retrieval-augmented generation (RAG) systems have been emerged as a promising direction to al-
leviate some challenges faced by large models (LMs), e.g., hallucinations (Mallen et al., 2023;
Khandelwal et al., 2020; Izacard et al., 2022). As shown in Figure 1a that large-scale documents in
these systems are typically segmented into a myriad of short document chunks that can be embedded
for retrieval. Upon the arrival of a user-input query, the most relevant chunks are then retrieved and
prepended to the input as an augmented query fed to an LM for prefill, followed by decoding in an
autoregressive (AR) manner to generate responses. RAG system effectively utilizes factual docu-
ments as supplementary data to enhance model’s ability to generate more accurate and contextually
rich responses, hence widely adopted by various applications, such as question answering (Siriward-
hana et al., 2023; Han et al., 2024) and content creation (Khattab et al., 2022), etc. However, existing
RAG systems come with several limitations from the system perspective.
First, repeatedly recalled document chunks require recomputation of the key-value (KV) caches,
leading to redundant computation. Second, the augmented document contains substantially more
tokens for prefill which contributes to considerably more computational overhead since the compu-
tation cost of KV caches is quadratic to the input sequence length. It, hence, significantly increases
TTFT, making RAG systems possibly unsuitable for applications that have stringent constraints on
response time. Third, as a side effect of the requirement in substantial computation resources for
concatenated document prefill, the batch size on a single device might be limited.
The fundamental reason for these issues lies in prefill paradigm of the current RAG system, which
involves online computation of the concatenated long documents, i.e. it collects the most relevant
documents and then performs prefill for them together. A natural question arises: can we alter this
∗Corresponding author. zhic@mthreads.com
†tangyaohua28@gmail.com
arXiv:2410.07590v1  [cs.CV]  10 Oct 2024
"
