"4
EXPERIMENTS
This section evaluates performance and accuracy of a number of TurboRAG model variants against
the conventional RAG models. Specifically, we seek to answer the questions below in this section:
• How does TurboRAG perform on document question-answering (QA)?
• What is the overall TTFT performance of TurboRAG compared against the N¨aive RAG system on
popular benchmarks?
• How large is the regression in the general capabilities of TurboRAG models?
• How efficient is TurboRAG in scaling inference batch sizes?
4.1
EXPERIMENT SETUP
We selected gpt-4o-2024-08-06 as the baseline due to its excellence in many benchmark suites. For
brevity, we refer the conventional RAG system as ”Na¨ıve RAG”. We also fine-tuned two models for
TurboRAG, namely TurboRAG-composite and TurboRAG-reordered corresponding to composite
positions and reordered positions, respectively. All three models are fine-tuned on a dataset com-
posed of 50% document QA data and 50% general tasks (e.g., code, dialogue, reasoning). All data
are publicly accessible. For a detailed composition of the dataset, please refer to Appendix B.
Training Setup We base our training on Qwen2-7B(Yang et al., 2024), performing SFT on the
aforementioned dataset. The fine-tuning was conducted on 32 NVIDIA A100 80GB GPUs with a
batch size of 256 sequences, using a learning rate of 1e-5 and the AdamW optimizer(Loshchilov,
2017). Both Na¨ıve RAG and TurboRAG models were trained using the same data proportions to
ensure comparability.
4.2
DOCUMENT QA ACCURACY
Let’s first evaluate the accuracy of document QA via intensive study on RGB Benchmark(Chen et al.,
2024b), a bilingual benchmark designed to test a model’s ability to answer questions on retrieved
documents. We followed the testing methodology provided by the official guidelines and let each
query extract five documents during the evaluation. In addition, we also measured the accuracy with
varying noise levels from 0.2 to 0.8 (e.g., Noise Ratio = 0.6 means 3 out of 5 retrieved documents
are irrelevant or noisy). In order reveal the effectiveness of fine-tuning, we gauged accuracy of each
TurboRAG configuration with and without fine-tuning.
As shown in Table 1, without fine-tuning, the accuracy drops significantly. Particularly, as the task
difficulty increases (i.e., with a higher noise ratio), the accuracy can decline by nearly 20%. This is
because the RAG models never learned the behavior of the new independent attention and composite
positions employed in inference. Nonetheless, simply fine-tuning the model with the small dataset
enables the TurboRAG models to attain impressive accuracy. Compared to the N¨aive RAG, even
without fine-tuning, independent attention and reordered positions only decrease the average ac-
curacy by 5.8% (96.8 vs 91.0) and 4.2% (96.8 vs 92.6). After fine-tuning, TurboRAG-reordered
and TurboRAG-composite can effectively maintain the benchmark accuracy gap within 1% com-
pared to the Na¨ıve RAG. They also demonstrated comparable performance to GPT-4o across both
Chinese and English datasets even under high-noise conditions. This highlights the effectiveness of
the proposed modifications in preserving high accuracy when leveraging KV cache in document QA
tasks.
To validate that our method proposed techniques are also directly applicable to long text input cases,
we inspected TurboRAG’s accuracy on an additional long-text RAG benchmark dataset, Long-
Bench(Bai et al., 2023). As shown in Table 2, TurboRAG also exhibits comparable answer accuracy
to that of Na¨ıve RAG in such use scenarios.
In all experiments, the performance of TurboRAG-composite was consistently inferior to that of
TurboRAG-reordered, particularly in more challenging contexts such as LongBench. This observa-
tion further validates the necessity of maintaining the accuracy of relative positional differences in
positional encoding.
"
