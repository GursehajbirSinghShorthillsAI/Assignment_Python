"B
DATA PROPORTIONS
Table 5: Sampling Ratios of Different Data Types during Model Fine-tuning
Data Type
Sampling Ratio
Document Q&A
50%
General Dialogue
25%
Reasoning
10%
Code
10%
Others
5%
Table 6: Specific Data and Quantities of Document Q&A
Data Name
Language
Quantity
glave-rag-v1
English
51,153
CovidQA
English
1,519
E-Manual
English
1,186
PubMedQA
English
22,050
MS Marco
English
2,267
FinQA
English
14,268
ExpertQA
English
1,824
HotpotQA
English
17,796
TechQA
English
1,496
HAGRID
English
3,214
DelusionQA
English
1,642
BioASQ
English
4,619
CUAD
English
2,040
TAT-QA
English
29,766
BaiduSTI
Chinese
4,032
DuReader
Chinese
10,000
BaiduBaike
Chinese
13,615
Wiki
Chinese
9,265
C
COMPUTATIONAL LOAD CALCULATION
Here, we present the method for calculating FLOPS, while omitting the computation of lm head due
to its relatively small proportion. Let the number of input tokens be denoted as ninput and the context
length as ncontext. For a LLM utilizing the Swiglu activation function, the relevant parameters include
layer num, head num, kv head num, head size, hidden size, and intermediate size. For each token:
• The computational cost of the QKV transformation for each layer, denoted as Cqkv, is given
by:
Cqkv = 2 × hidden size × (head num + 2 × kv head num) × head size
• The computational cost of the attention mechanism for each layer, denoted as Cattn, is
expressed as:
Cattn = 2 × head num × head size × ncontext
• The computational cost of the projection following the attention mechanism for each layer,
denoted as Co, is given by:
Co = 2 × hidden size2
"
